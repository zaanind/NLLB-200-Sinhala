{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnsWYxL52DFK"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece transformers==4.33 datasets sacremoses sacrebleu -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYHsfVj72QyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CDMQAyg72dgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import datasets\n",
        "from tqdm.auto import tqdm, trange\n",
        "import numpy as np\n",
        "from transformers import NllbTokenizer\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"zaanind/sinhala_englsih_nmt\")\n",
        "\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "\n",
        "\n",
        "en = train_dataset[\"english\"]\n",
        "si = train_dataset[\"sinhala\"]"
      ],
      "metadata": {
        "id": "TLvpBW2P2dzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LBvgnAnqqHbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OvKkxZxqqHXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vzEnh7KS3bZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "model_name = \"zaanind/nllb-ensi-v1-tuning\" #\"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "yHYQxEqs3bwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-Gr54xk2OUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.src_lang = \"eng_Latn\"\n",
        "inputs = tokenizer(text=\"i'm glad i could make it to your birthday event it was such a memorable experience\", return_tensors=\"pt\")\n",
        "translated_tokens = model.generate(\n",
        "    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"sin_Sinh\"]\n",
        ")\n",
        "print(tokenizer.decode(translated_tokens[0], skip_special_tokens=True))\n",
        "# The fields were lit by the morning sun"
      ],
      "metadata": {
        "id": "Jd8afixA46XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kStyojo45_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.optimization import Adafactor\n",
        "from transformers import get_constant_schedule_with_warmup\n",
        "model.cuda();\n",
        "optimizer = Adafactor(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    scale_parameter=False,\n",
        "    relative_step=False,\n",
        "    lr=1e-4,\n",
        "    clip_threshold=1.0,\n",
        "    weight_decay=1e-3,\n",
        ")\n",
        "scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)"
      ],
      "metadata": {
        "id": "Wn9rHWiO6dIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def get_batch_pairs(batch_size, dataset=dataset):\n",
        "    xx, yy = [], []\n",
        "    for _ in range(batch_size):\n",
        "        item = dataset['train'][random.randint(0, len(dataset['train']) - 1)]\n",
        "        xx.append(item[\"english\"])\n",
        "        yy.append(item[\"sinhala\"])\n",
        "    return xx, yy\n",
        "\n",
        "print(get_batch_pairs(1))"
      ],
      "metadata": {
        "id": "Ukeph-386dF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "def cleanup():\n",
        "    \"\"\"Try to free GPU memory\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "nSj15Yuh6pd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 18  # 32 already doesn't fit well to 15GB of GPU memory\n",
        "max_length = 128  # token sequences will be truncated\n",
        "training_steps = 60000  # Usually, I set a large number of steps,\n",
        "# and then just interrupt the training manually\n",
        "losses = []  # with this list, I do very simple tracking of average loss\n",
        "MODEL_SAVE_PATH = '/content/models'  # on my Google drive"
      ],
      "metadata": {
        "id": "5sorZX3Z6sg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJcpGW4kAH13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "x, y, loss = None, None, None\n",
        "cleanup()\n",
        "\n",
        "tq = trange(len(losses), training_steps)\n",
        "for i in tq:\n",
        "    xx, yy = get_batch_pairs(batch_size)\n",
        "    try:\n",
        "        tokenizer.src_lang = \"eng_Latn\"\n",
        "        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n",
        "        tokenizer.src_lang = \"sin_Sinh\"\n",
        "        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n",
        "        # -100 is a magic value ignored in the loss function\n",
        "        # because we don't want the model to learn to predict padding ids\n",
        "        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        loss = model(**x, labels=y.input_ids).loss\n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        scheduler.step()\n",
        "\n",
        "    except RuntimeError as e:  # usually, it is out-of-memory\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        x, y, loss = None, None, None\n",
        "        cleanup()\n",
        "        print('error', max(len(s) for s in xx + yy), e)\n",
        "        continue\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        # each 1000 steps, I report average loss at these steps\n",
        "        print(i, np.mean(losses[-1000:]))\n",
        "\n",
        "    if i % 50 == 0 and i > 0:\n",
        "        model.save_pretrained(MODEL_SAVE_PATH)\n",
        "        tokenizer.save_pretrained(MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "_xPhDMJI6pbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FHnooxuJJv6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import NllbTokenizer, AutoModelForSeq2SeqLM\n",
        "model_load_name = '/gd/MyDrive/models/nllb-rus-tyv-v1'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_load_name).cuda()\n",
        "tokenizer = NllbTokenizer.from_pretrained(model_load_name)\n"
      ],
      "metadata": {
        "id": "MMAauxccJv3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(\n",
        "    text, src_lang='rus_Cyrl', tgt_lang='eng_Latn',\n",
        "    a=32, b=3, max_input_length=1024, num_beams=4, **kwargs\n",
        "):\n",
        "    \"\"\"Turn a text or a list of texts into a list of translations\"\"\"\n",
        "    tokenizer.src_lang = src_lang\n",
        "    tokenizer.tgt_lang = tgt_lang\n",
        "    inputs = tokenizer(\n",
        "        text, return_tensors='pt', padding=True, truncation=True,\n",
        "        max_length=max_input_length\n",
        "    )\n",
        "    model.eval() # turn off training mode\n",
        "    result = model.generate(\n",
        "        **inputs.to(model.device),\n",
        "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
        "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
        "        num_beams=num_beams, **kwargs\n",
        "    )\n",
        "    return tokenizer.batch_decode(result, skip_special_tokens=True)\n",
        "\n",
        "# Example usage:\n",
        "t = 'мөңгүн үр чыткаш карарар'\n",
        "print(translate(t, 'tyv_Cyrl', 'rus_Cyrl'))\n",
        "# ['серебро от времени чернеет']"
      ],
      "metadata": {
        "id": "qZ8dil4jJ79Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSdI4u7sKDjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "id": "uUQvoBshKD94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upload_repo = \"zaanind/nllb-ensi-v1-tuning\"\n",
        "tokenizer.push_to_hub(upload_repo)\n",
        "model.push_to_hub(upload_repo)"
      ],
      "metadata": {
        "id": "ULmB3JeaKHs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ensure 1-20h trining time with 79lower loss"
      ],
      "metadata": {
        "id": "Ecrz9_DRMwSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hydkq0kSMwPC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}